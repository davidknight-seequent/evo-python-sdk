{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "## Update an existing drilling campaign object\n",
    "\n",
    "This example uses the Evo Python SDK and shows how to download an existing drilling campaign object, apply new data to it, and publish it as a new version of that object.\n",
    "\n",
    "### Requirements\n",
    "\n",
    "You must have a Seequent account with the Evo entitlement to use this notebook.\n",
    "\n",
    "The following parameters must be provided:\n",
    "\n",
    "- The client ID of your Evo application.\n",
    "- The callback/redirect URL of your Evo application.\n",
    "\n",
    "To obtain these app credentials, refer to the [Apps and tokens guide](https://developer.seequent.com/docs/guides/getting-started/apps-and-tokens) in the Seequent Developer Portal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from evo.notebooks import FeedbackWidget, ServiceManagerWidget\n",
    "\n",
    "cache_location = \"data\"\n",
    "input_path = f\"{cache_location}/input\"\n",
    "\n",
    "# Evo app credentials\n",
    "client_id = \"<your-client-id>\"  # Replace with your client ID\n",
    "redirect_url = \"<your-redirect-url>\"  # Replace with your redirect URL\n",
    "\n",
    "manager = await ServiceManagerWidget.with_auth_code(\n",
    "    redirect_url=redirect_url,\n",
    "    client_id=client_id,\n",
    "    cache_location=cache_location,\n",
    ").login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### Use the Evo Python SDK to create an object client and a data client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evo.objects import ObjectAPIClient\n",
    "\n",
    "# The object client will manage your auth token and Geoscience Object API requests.\n",
    "object_client = ObjectAPIClient(manager.get_environment(), manager.get_connector())\n",
    "\n",
    "# The data client will manage saving your data as Parquet and publishing your data to Evo storage.\n",
    "data_client = object_client.get_data_client(manager.cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### Define helper functions\n",
    "\n",
    "These functions assist with assembling the elements and components of geoscience objects and for viewing the new object in the Evo portal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def create_hole_id_mapping(hole_id_table, value_list):\n",
    "    \"\"\"\n",
    "    Create a hole ID mapping table based on the hole ID table and the value list.\n",
    "\n",
    "    Args:\n",
    "        hole_id_table (pd.DataFrame): The hole ID lookup table.\n",
    "        value_list (pd.DataFrame): The value list to create the mapping from.\n",
    "\n",
    "    Returns:\n",
    "        mapping_df (pd.DataFrame): The hole ID mapping table.\n",
    "    \"\"\"\n",
    "\n",
    "    num_keys = len(hole_id_table.index)\n",
    "\n",
    "    mapping_df = pd.DataFrame(list())\n",
    "    mapping_df[\"hole_index\"] = hole_id_table[\"key\"]\n",
    "    mapping_df[\"offset\"] = [0] * num_keys\n",
    "    mapping_df[\"count\"] = [0] * num_keys\n",
    "\n",
    "    mapping_df[\"hole_index\"] = mapping_df[\"hole_index\"].astype(\"int32\")\n",
    "    mapping_df[\"offset\"] = mapping_df[\"offset\"].astype(\"uint64\")\n",
    "    mapping_df[\"count\"] = mapping_df[\"count\"].astype(\"uint64\")\n",
    "\n",
    "    prev_value = \"\"\n",
    "    key = \"\"\n",
    "    count = 0\n",
    "    offset = 0\n",
    "\n",
    "    for index, row in value_list.iterrows():\n",
    "        new_value = row[\"data\"]\n",
    "\n",
    "        if new_value != prev_value:\n",
    "            if prev_value != \"\":\n",
    "                mapping_df.loc[mapping_df[\"hole_index\"] == key, \"count\"] = count\n",
    "                mapping_df.loc[mapping_df[\"hole_index\"] == key, \"offset\"] = offset\n",
    "                offset += count\n",
    "\n",
    "            mask = hole_id_table[\"value\"] == new_value\n",
    "            masked_df = hole_id_table[mask]\n",
    "            try:\n",
    "                key_row = masked_df.iloc[[0]]\n",
    "            except IndexError:\n",
    "                print(\"Ignoring this hole ID\")\n",
    "                continue\n",
    "\n",
    "            key = key_row[\"key\"].iloc[0]\n",
    "            count = 1\n",
    "            prev_value = new_value\n",
    "        else:\n",
    "            count += 1\n",
    "\n",
    "    mapping_df.loc[mapping_df[\"hole_index\"] == key, \"count\"] = count\n",
    "    mapping_df.loc[mapping_df[\"hole_index\"] == key, \"offset\"] = offset\n",
    "\n",
    "    return mapping_df\n",
    "\n",
    "\n",
    "def create_category_lookup_and_values(attribute):\n",
    "    \"\"\"\n",
    "    Create a category lookup table and the associated column of mapped key values.\n",
    "\n",
    "    Args:\n",
    "        attribute (pd.DataFrame): An attribute of a geoscience object.\n",
    "\n",
    "    Returns:\n",
    "        table_df (pd.DataFrame): The category lookup table.\n",
    "        values_df (pd.DataFrame): The associated column with mapped key values.\n",
    "    \"\"\"\n",
    "\n",
    "    # Replace NaN with empty string\n",
    "    attribute.replace(np.nan, \"\", regex=True, inplace=True)\n",
    "    set_obj = set(attribute[\"data\"])\n",
    "    list_obj = list(set_obj)\n",
    "    list_obj.sort()\n",
    "    num_unique_elements = len(list_obj)\n",
    "\n",
    "    # Create lookup table\n",
    "    table_df = pd.DataFrame([])\n",
    "    table_df[\"key\"] = list(range(0, num_unique_elements))\n",
    "    table_df[\"value\"] = list_obj\n",
    "\n",
    "    # Create data column\n",
    "    values_df = pd.DataFrame([])\n",
    "    values_df[\"data\"] = attribute[\"data\"].map(table_df.set_index(\"value\")[\"key\"])\n",
    "    return table_df, values_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Select the drilling-campaign object to be updated\n",
    "\n",
    "The example below uses a filter to only show relevant objects in the workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "all_objects = await object_client.list_all_objects()\n",
    "\n",
    "table = PrettyTable([\"ID\", \"Name\", \"Schema\"])\n",
    "for index, obj in enumerate(all_objects):\n",
    "    if \"drilling-campaign\" in obj.schema_id.sub_classification:\n",
    "        table.add_row([str(obj.id).ljust(40), obj.name.ljust(40), obj.schema_id.sub_classification.ljust(40)])\n",
    "\n",
    "if len(table.rows) == 0:\n",
    "    print(\"No drilling campaigns found. Publish a drilling campaign using the 'publish-drilling-campaign' notebook.\")\n",
    "else:\n",
    "    print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### Download the object files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "object_id = \"3824d4a5-586d-4a3c-88e5-14eb6dc9dd9d\"  # Replace with your selected object ID\n",
    "\n",
    "downloaded_object = await object_client.download_object_by_id(object_id=object_id)\n",
    "\n",
    "metadata = downloaded_object.metadata\n",
    "downloaded_dict = downloaded_object.as_dict()\n",
    "\n",
    "\n",
    "def download_table(table_info, fb=None):\n",
    "    if fb is None:\n",
    "        fb = FeedbackWidget(\"Downloading unknown table\")\n",
    "    return data_client.download_table(\n",
    "        object_id=metadata.id, version_id=metadata.version_id, table_info=table_info, fb=fb\n",
    "    )\n",
    "\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# Use the data client to download the coordinate data.\n",
    "hole_indices = (\n",
    "    await download_table(\n",
    "        downloaded_dict[\"hole_id\"][\"values\"],\n",
    "        fb=FeedbackWidget(f\"Downloading hole indices data as '{downloaded_dict['hole_id']['values']['data']}'\"),\n",
    "    )\n",
    ").to_pandas()\n",
    "index_to_name_map = (\n",
    "    await download_table(\n",
    "        downloaded_dict[\"hole_id\"][\"table\"],\n",
    "        fb=FeedbackWidget(\n",
    "            f\"Downloading hole index to name map data as '{downloaded_dict['hole_id']['table']['data']}'\"\n",
    "        ),\n",
    "    )\n",
    ").to_pandas()\n",
    "names = pd.DataFrame({\"key\": hole_indices[\"data\"]}).merge(index_to_name_map, on=\"key\", how=\"left\")[\"value\"]\n",
    "\n",
    "collar_locations = (\n",
    "    await download_table(\n",
    "        downloaded_dict[\"planned\"][\"collar\"][\"coordinates\"],\n",
    "        fb=FeedbackWidget(\n",
    "            f\"Downloading collar locations data as '{downloaded_dict['planned']['collar']['coordinates']['data']}'\"\n",
    "        ),\n",
    "    )\n",
    ").to_pandas()\n",
    "hole_lengths = (\n",
    "    await download_table(\n",
    "        downloaded_dict[\"planned\"][\"collar\"][\"distances\"],\n",
    "        fb=FeedbackWidget(\n",
    "            f\"Downloading collar hole distances table as '{downloaded_dict['planned']['collar']['distances']['data']}'\"\n",
    "        ),\n",
    "    )\n",
    ").to_pandas()\n",
    "chunk_data = (\n",
    "    await download_table(\n",
    "        downloaded_dict[\"planned\"][\"collar\"][\"holes\"],\n",
    "        fb=FeedbackWidget(\n",
    "            f\"Downloading collar chunks data as '{downloaded_dict['planned']['collar']['holes']['data']}'\"\n",
    "        ),\n",
    "    )\n",
    ").to_pandas()\n",
    "\n",
    "attributes = []\n",
    "# Use the data client to download the attribute data and merge it with the coordinates data.\n",
    "for attribute in downloaded_dict[\"planned\"][\"collar\"][\"attributes\"]:\n",
    "    attribute_name = attribute[\"name\"]\n",
    "    attribute_type = attribute[\"attribute_type\"]\n",
    "\n",
    "    # Download the attribute data. Every attribute has a 'values' data file.\n",
    "    values_data = (\n",
    "        await data_client.download_table(\n",
    "            object_id=metadata.id,\n",
    "            version_id=metadata.version_id,\n",
    "            table_info=attribute[\"values\"],\n",
    "            fb=FeedbackWidget(\n",
    "                f\"Downloading attribute '{attribute_name}' values data as '{attribute['values']['data']}'\"\n",
    "            ),\n",
    "        )\n",
    "    ).to_pandas()\n",
    "    attributes.append(values_data)\n",
    "\n",
    "df = pd.concat([names, collar_locations, hole_lengths, chunk_data, *attributes], axis=1)\n",
    "\n",
    "path_data = (\n",
    "    await download_table(\n",
    "        downloaded_dict[\"planned\"][\"path\"],\n",
    "        fb=FeedbackWidget(f\"Downloading collar path data as '{downloaded_dict['planned']['path']['data']}'\"),\n",
    "    )\n",
    ").to_pandas()\n",
    "\n",
    "processed_path_data = pd.concat(\n",
    "    [\n",
    "        path_data.iloc[start : start + length].reset_index(drop=True)\n",
    "        for start, length in zip(chunk_data[\"offset\"], chunk_data[\"count\"])\n",
    "    ],\n",
    "    axis=0,\n",
    ").reset_index(drop=True)\n",
    "hole_name = []\n",
    "\n",
    "for hole_id, count in zip(chunk_data[\"hole_index\"], chunk_data[\"count\"]):\n",
    "    hole_name.extend([index_to_name_map[index_to_name_map[\"key\"] == hole_id][\"value\"].values[0]] * count)\n",
    "processed_path_data[\"hole_name\"] = hole_name\n",
    "processed_path_data = processed_path_data[\n",
    "    [\"hole_name\"] + [col for col in processed_path_data.columns if col != \"hole_name\"]\n",
    "]\n",
    "\n",
    "attribute_tables = {}\n",
    "# Use the data client to download the attribute data and merge it with the coordinates data.\n",
    "if \"collections\" in downloaded_dict[\"planned\"]:\n",
    "    for attribute_table in downloaded_dict[\"planned\"][\"collections\"]:\n",
    "        collection_type = attribute_table[\"collection_type\"]\n",
    "        collection_name = f\"planned_{collection_type}_({attribute_table['name']})\"\n",
    "\n",
    "        if collection_type == \"interval\":\n",
    "            distance_container = attribute_table[\"from_to\"][\"intervals\"][\"start_and_end\"]\n",
    "            attribute_container = attribute_table[\"from_to\"][\"attributes\"]\n",
    "            distance_data = (\n",
    "                await data_client.download_table(\n",
    "                    object_id=metadata.id,\n",
    "                    version_id=metadata.version_id,\n",
    "                    table_info=distance_container,\n",
    "                    fb=FeedbackWidget(f\"Downloading distance data as '{distance_container['data']}'\"),\n",
    "                )\n",
    "            ).to_pandas()\n",
    "        elif collection_type == \"distance\":\n",
    "            distance_container = attribute_table[\"intervals\"][\"start_and_end\"]\n",
    "            attribute_container = attribute_table[\"distance\"][\"attributes\"]\n",
    "            distance_data = (\n",
    "                await data_client.download_table(\n",
    "                    object_id=metadata.id,\n",
    "                    version_id=metadata.version_id,\n",
    "                    table_info=distance_container,\n",
    "                    fb=FeedbackWidget(f\"Downloading distance data as '{distance_container['data']}'\"),\n",
    "                )\n",
    "            ).to_pandas()\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        attribute_chunk_data = (\n",
    "            await data_client.download_table(\n",
    "                object_id=metadata.id,\n",
    "                version_id=metadata.version_id,\n",
    "                table_info=attribute_table[\"holes\"],\n",
    "                fb=FeedbackWidget(f\"Downloading attribute chunk data as '{attribute_table['holes']['data']}'\"),\n",
    "            )\n",
    "        ).to_pandas()\n",
    "\n",
    "        columns = [distance_data]\n",
    "        for column in attribute_container:\n",
    "            attribute_name = column[\"name\"]\n",
    "            attribute_type = column[\"attribute_type\"]\n",
    "\n",
    "            column_data = (\n",
    "                await data_client.download_table(\n",
    "                    object_id=metadata.id,\n",
    "                    version_id=metadata.version_id,\n",
    "                    table_info=column[\"values\"],\n",
    "                    fb=FeedbackWidget(\n",
    "                        f\"Downloading attribute '{attribute_name}' column data as '{column['values']['data']}'\"\n",
    "                    ),\n",
    "                )\n",
    "            ).to_pandas()\n",
    "            column_data.columns = [\"data\"]\n",
    "\n",
    "            # If the attribute is a category, download the 'table' data as well.\n",
    "            if attribute_type == \"category\":\n",
    "                lookup_table = (\n",
    "                    await data_client.download_table(\n",
    "                        object_id=metadata.id,\n",
    "                        version_id=metadata.version_id,\n",
    "                        table_info=column[\"table\"],\n",
    "                        fb=FeedbackWidget(\n",
    "                            f\"Downloading attribute '{attribute_name}' lookup table data as '{column['table']['data']}'\"\n",
    "                        ),\n",
    "                    )\n",
    "                ).to_pandas()\n",
    "\n",
    "                # Merge the values data with the table data.\n",
    "                merged_data = pd.merge(column_data, lookup_table, left_on=\"data\", right_on=\"key\", how=\"left\")\n",
    "                # Drop the 'data' and 'key' columns from the merged data.\n",
    "                merged_data.drop(columns=[\"data\", \"key\"], inplace=True)\n",
    "                # Rename the 'value' column to the attribute name.\n",
    "                merged_data.rename(columns={\"value\": attribute_name}, inplace=True)\n",
    "                # Concatenate the merged data with the coordinates data.\n",
    "                columns.append(merged_data)\n",
    "\n",
    "            elif attribute_type == \"scalar\":\n",
    "                # Rename the 'data' column to the attribute name.\n",
    "                column_data.rename(columns={\"data\": attribute_name}, inplace=True)\n",
    "                # Concatenate the data with the coordinates data.\n",
    "                columns.append(column_data)\n",
    "\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown attribute type: {attribute_type}\")\n",
    "\n",
    "        attribute_tables[collection_name] = pd.concat(columns, axis=1)\n",
    "        attribute_tables[f\"{collection_name} (Cnk)\"] = attribute_chunk_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "samples",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
